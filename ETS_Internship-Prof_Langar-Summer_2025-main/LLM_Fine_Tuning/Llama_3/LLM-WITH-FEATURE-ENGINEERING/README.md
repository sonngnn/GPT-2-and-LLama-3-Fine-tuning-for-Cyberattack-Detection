## Advanced Configuration Parameters

### Memory Management
- `--per_feature_mem_mb`: Memory per preprocessing task (default: 2000 MB)
- `--per_train_mem_mb`: Memory per training task (default: 4000 MB)  
- `--reserve_system_pct`: Memory to reserve for system (default: 20%)

### Execution Control
- `--features_list`: List of feature counts to process (required)
- `--max_retries`: Maximum retries for failed tasks (default: 2)
- `--train_epochs`: Number of training epochs (default: 3)
- `--preprocessing_only`: Run preprocessing only, skip training
- `--dry_run`: Test configuration without executing actual tasks

### Example Advanced Configurations

**Conservative (Low Memory System):**
```bash
python orchestrate_pipeline.py \
  --features_list 8 12 \
  --per_feature_mem_mb 1500 \
  --per_train_mem_mb 3000 \
  --reserve_system_pct 30
```

**Aggressive (High Memory System):**
```bash
python orchestrate_pipeline.py \
  --features_list 8 12 16 20 24 \
  --per_feature_mem_mb 3000 \
  --per_train_mem_mb 6000 \
  --reserve_system_pct 15
```

**Preprocessing Only (Generate datasets for later training):**
```bash
python orchestrate_pipeline.py \
  --features_list 8 12 16 \
  --preprocessing_only
```# Multi-Feature LLaMA Network Traffic Classification Pipeline

An orchestrated pipeline for training LLaMA models with different feature engineering configurations in parallel, with intelligent memory management and robust error handling.

## Overview

This pipeline automates the generation of multiple feature datasets (8 features, 12 features, etc.) and trains corresponding LLaMA models in parallel while managing system resources efficiently. It's designed for comparative studies of how different feature counts affect model performance.

## Project Structure

```
.
├── README.md
├── orchestrate_pipeline.py       # Main orchestration script
├── data_preprocessing.py          # Enhanced data preprocessing with feature engineering
├── feature_engineering.py        # Feature engineering module (enhanced)
├── llama_finetuning.py           # Original LLaMA training script (unchanged)
├── evaluate_model.py             # Model evaluation script
├── inference.py                  # Inference script
├── run_train.sh                  # Training wrapper script
├── setup_config.py               # Automatic configuration script
├── test_pipeline.py              # System testing and validation
├── run_pipeline.sh               # Interactive pipeline runner
├── run_orchestrated.sh           # Generated: Custom SLURM script
├── pipeline_config.json          # Generated: Configuration file
├── requirements.txt              # Python dependencies
├── data/                         # Input and processed datasets
│   ├── data_8_features/         # Generated: 8-feature dataset
│   ├── data_12_features/        # Generated: 12-feature dataset
│   └── your_dataset.csv         # Your input dataset (path configured)
├── logs/                        # Execution logs
│   ├── logs_8_features/         # Generated: logs for 8-feature processing
│   ├── logs_12_features/        # Generated: logs for 12-feature processing
│   └── ...
└── outputs/                     # Trained models and results
    ├── outputs_8_features/      # Generated: 8-feature model
    ├── outputs_12_features/     # Generated: 12-feature model
    └── ...
```

## Prerequisites

### System Requirements
- Python 3.8+
- CUDA-compatible GPU (recommended)
- Minimum 8GB RAM (16GB+ recommended for parallel execution)
- 50GB+ free disk space

### Dependencies
Install required packages:
```bash
pip install -r requirements.txt
```

### Data Requirements
- CSV file with network traffic data
- **Must contain a 'label' column** for classification
- Columns will be automatically detected and validated by `setup_config.py`
- Compatible with CICIoT, UNSW-NB15, CIC-IDS2017, and similar network datasets

## Quick Start

### First-Time Setup (REQUIRED)
Configure the pipeline for your dataset:
```bash
python setup_config.py --interactive
```
This will:
- Analyze your dataset structure
- Update configuration files automatically
- Generate a customized SLURM script
- Suggest appropriate feature counts

### Test Your Configuration
Validate system requirements and dependencies:
```bash
python test_pipeline.py --test_all
```

### Run the Pipeline

**For SLURM clusters:**
```bash
sbatch run_orchestrated.sh  # Generated by setup_config.py
```

**For local execution:**
```bash
python orchestrate_pipeline.py --features_list 8 12 --per_feature_mem_mb 2000 --per_train_mem_mb 4000
```

**Interactive guided setup:**
```bash
./run_pipeline.sh  # Interactive mode with guided setup
```

### Dry Run (Test Configuration)
Test your configuration without executing actual tasks:
```bash
python orchestrate_pipeline.py --features_list 8 12 16 --dry_run
```

## Configuration and Setup

### Automatic Configuration (Recommended)

**Step 1: Configure your dataset**
```bash
python setup_config.py --interactive
```
The configuration script will:
- Analyze your CSV dataset structure
- Check column compatibility with the pipeline
- Suggest optimal feature counts based on your data
- Update all necessary configuration files
- Generate a customized SLURM script for your environment

**Step 2: Test your setup**
```bash
python test_pipeline.py --test_all
```

**Step 3: Run the pipeline**
```bash
sbatch run_orchestrated.sh  # Submit to SLURM cluster
# OR
python orchestrate_pipeline.py --features_list 8 12  # Run locally
```

### Manual Configuration

If you prefer manual setup:

1. **Update dataset path** in `data_preprocessing.py` (line ~270):
```python
default="/path/to/your/dataset.csv"
```

2. **Verify dataset requirements**:
- Must contain a 'label' column
- Should have network traffic features (flow_duration, Protocol Type, etc.)

3. **Customize SLURM script**:
- Update account, email, and resource requirements in `run_orchestrated_slurm.sh`

4. **Choose feature counts** based on your dataset size:
- Small datasets (<20 features): Try 6, 8, 10
- Medium datasets (20-50 features): Try 8, 12, 16, 20  
- Large datasets (50+ features): Try 8, 12, 16, 20, 24

## Individual Script Usage

### Data Preprocessing
Process data with specific feature count:
```bash
python data_preprocessing.py --n_features 8 --out_dir data/data_8_features
```

### Feature Engineering
Run feature engineering independently:
```bash
python feature_engineering.py --n_features 8 --input_csv data.csv --output_dir results/
```

### Model Training
Train a single model configuration:
```bash
./run_train.sh --data_dir data/data_8_features --output_dir outputs/outputs_8_features
```

### Model Evaluation
Evaluate a trained model:
```bash
python evaluate_model.py  # Uses default checkpoint path
```

## Output Structure

After successful execution, you'll find:

### Generated Datasets
```
data/
├── data_8_features/
│   ├── train.json
│   ├── validation.json
│   ├── test.json
│   └── dataset_metadata.json
└── data_12_features/
    ├── train.json
    ├── validation.json
    ├── test.json
    └── dataset_metadata.json
```

### Trained Models
```
outputs/
├── outputs_8_features/
│   ├── checkpoint-2000/
│   ├── checkpoint-4000/
│   ├── adapter_config.json
│   ├── adapter_model.safetensors
│   └── training_summary.json
└── outputs_12_features/
    ├── checkpoint-2000/
    ├── checkpoint-4000/
    ├── adapter_config.json
    ├── adapter_model.safetensors
    └── training_summary.json
```

### Execution Logs
```
logs/
├── logs_8_features/
│   ├── preprocessing_8_features.log
│   ├── training_8_features.log
│   └── status_preprocessing.json
└── logs_12_features/
    ├── preprocessing_12_features.log
    ├── training_12_features.log
    └── status_training.json
```

## Advanced Features

### Automatic Checkpoint Recovery
The pipeline automatically detects and resumes from existing checkpoints:
- Prevents data loss from interrupted executions
- Supports incremental processing
- Maintains training state across restarts

### Resource Monitoring
Real-time memory usage monitoring:
- Prevents system overload
- Adaptive task scheduling
- Graceful degradation under resource constraints

### Comprehensive Logging
Detailed logging for debugging and monitoring:
- Per-task log files
- Status tracking with JSON metadata
- Error reporting with stack traces
- Execution summaries

## Troubleshooting

### Common Issues

**Configuration Problems:**
```bash
# Dataset not found
python setup_config.py --csv_path /correct/path/to/your/data.csv

# Column compatibility issues
# Use setup_config.py to analyze your dataset structure
python setup_config.py --interactive
```

**Memory Errors:**
```bash
# Reduce concurrent tasks
python orchestrate_pipeline.py --features_list 8 --per_train_mem_mb 2000
```

**CUDA Out of Memory:**
```bash
# Train one model at a time
python orchestrate_pipeline.py --features_list 8 --per_train_mem_mb 6000
```

**Missing Dependencies:**
```bash
# Install all requirements
pip install -r requirements.txt

# Test dependencies
python test_pipeline.py --test_dependencies
```

### Log Analysis
Check task-specific logs for detailed error information:
```bash
# View preprocessing logs
cat logs/logs_8_features/preprocessing_8_features.log

# View training logs  
cat logs/logs_8_features/training_8_features.log

# Check status files
cat logs/logs_8_features/status_preprocessing.json
```

### Recovery from Failures
The pipeline supports partial recovery:
```bash
# Resume failed training tasks only
python orchestrate_pipeline.py --features_list 12 16 --preprocessing_only
# Then run training separately
```

## Performance Optimization

### Memory Optimization
- Use feature engineering to reduce dataset size
- Adjust batch sizes in `llama_finetuning.py`
- Enable gradient checkpointing
- Use mixed precision training

### Compute Optimization
- Use `eval_steps=5000` for faster training
- Reduce validation set size
- Enable early stopping
- Use multiple GPUs if available

### Storage Optimization
- Use `save_total_limit=1` for minimal checkpoint storage
- Compress datasets after generation
- Use symbolic links for shared data

## Model Comparison

After training multiple configurations, compare results:

```bash
# Evaluate all models
for dir in outputs/outputs_*_features; do
    echo "Evaluating $dir"
    python evaluate_model.py --model_path "$dir"
done
```

### Expected Results Structure
```
Feature Count | Accuracy | F1-Score | Training Time | Model Size
8 features    | 91.2%    | 0.912    | 2.3h         | 145MB
12 features   | 93.1%    | 0.931    | 3.1h         | 145MB  
16 features   | 93.8%    | 0.938    | 3.8h         | 145MB
```

## Contributing

### Adding New Feature Configurations
1. Modify `feature_engineering.py` to support new feature counts
2. Update configuration ranges in `orchestrate_pipeline.py`
3. Test with `--dry_run` before full execution

### Extending Evaluation Metrics
1. Update `evaluate_model.py` with new metrics
2. Modify output formats as needed
3. Update README with new result interpretations

## License

This project builds upon existing LLaMA and Transformers libraries. Ensure compliance with respective licenses for commercial use.

## Support and Troubleshooting

For issues and questions:

1. **First-Time Setup Issues:**
   - Run `python setup_config.py --interactive` to reconfigure
   - Verify your dataset has a 'label' column
   - Check that your CSV file is accessible

2. **System Compatibility:**
   - Run `python test_pipeline.py --test_all` for comprehensive diagnostics
   - Check memory and disk space requirements
   - Verify all dependencies are installed

3. **Execution Problems:**
   - Check logs in `logs/` directories for detailed error messages
   - Review error messages in console output
   - Test with `--dry_run` to validate configuration

4. **Performance Issues:**
   - Adjust memory allocation parameters
   - Reduce concurrent tasks for low-memory systems
   - Use preprocessing-only mode for large datasets

5. **Dataset Compatibility:**
   - Use `setup_config.py` to analyze your dataset structure
   - Refer to feature engineering documentation
   - Check column naming conventions